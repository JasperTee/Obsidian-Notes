## 1. Types of Data

### **1.1 Structured Data**
![[Structured Data.png]]
- **Definition**:  
    Data stored in aÂ **predefined tabular format**Â with rows and columns, typically inÂ **relational databases**.
    
- **Characteristics**:
    
    - Each row is a record, each column holds a specific attribute.
        
    - Schema (structure) is fixed.
        
    - Querying is done withÂ **SQL**.
        
- **Lecture Example**:  
    Table of journal articles: Title, Journal, Volume, Issue, Date, Pages.
    
- **Advantages**:
    
    - High accuracy and consistency.
        
    - Easy to maintain, backup, and integrate.
        
    - Mature tools and strong query support.
        
- **Limitations**:
    
    - Rigid structure, hard to handle rapidly changing data formats.
        
- **Applications**:
    
    - Banking transactions, retail sales data, employee records.
        

---

### **1.2 Semi-structured Data**
![[Semi-structured data.png]]
- **Definition**:  
    Data that doesnâ€™t fit perfectly into tables but hasÂ **tags, markers, or keys**Â that make it self-describing.
    
- **Common Formats**: XML, JSON, YAML.
    
- **Characteristics**:
    
    - Flexible schema, not all records must have the same attributes.
        
    - Self-describing â€” each field has its own label.
- **Advantages**:
    
    - Easier to evolve as needs change.
        
    - Works well for data exchange between systems.
        
- **Limitations**:
    
    - Slower queries compared to structured data.
        
    - Needs parsing and validation for consistency.
        
- **Applications**:
    
    - API responses, system configurations, cross-platform data exchange.
        

---

### **1.3 Unstructured Data**
![[Unstructured Data.png]]
- **Definition**:  
    Data without a predefined data model or organization.
    
- **Examples**:
    
    - Emails, PDF files, Word documents.
        
    - Social media posts, chat messages.
        
    - Raw server logs.
        
- **Advantages**:
    
    - Can store any content type.
        
    - Rich in detail and context.
        
- **Limitations**:
    
    - Hard to search and analyze directly.
        
    - Often requires AI, NLP, or data preprocessing.
        
- **Applications**:
    
    - Sentiment analysis, document archiving, legal discovery.
        

---

### **1.4 Spatial Data**
![[Spatial Data.png]]
- **Definition**:  
    Data that includes a geographic or spatial component.
    
- **Examples**:
    
    - GIS (Geographic Information System) data.
        
    - GPS coordinates.
        
    - Satellite images, maps.
        
- **Advantages**:
    
    - Enables location-based insights and analysis.
        
- **Limitations**:
    
    - Large storage requirements.
        
    - Needs specialized tools (ArcGIS, PostGIS).
        
- **Applications**:
    
    - Logistics optimization, city planning, environmental monitoring.
        

---

### **1.5 Time Series Data**
![[Time Series Data.png]]
- **Definition**:  
    Data collected and stored over time to capture changes and trends.
    

#### **Type 1: Regular Interval Time Series**

- **Characteristics**:
    
    - Data recorded at consistent, fixed intervals.
        
    - Easy to handle with statistical methods and time series databases.
        
- **Lecture Examples**:
    
    - Hourly weather readings.
        
    - Daily closing stock prices.
        
    - Sensor readings every second.
        
- **Advantages**:
    
    - Easy to analyze and visualize.
        
    - Supports trend detection and forecasting.
        
- **Limitations**:
    
    - Can produce redundant data if no change occurs between intervals.
        

#### **Type 2: Irregular Interval Time Series**

- **Characteristics**:
    
    - Data recorded only when an event occurs.
        
    - Time gaps between data points are inconsistent.
        
- **Lecture Examples**:
    
    - GPS tracking when position changes.
        
    - Log entries on system errors.
        
    - Sensor reports when thresholds are exceeded.
        
- **Advantages**:
    
    - Efficient storage and transmission.
        
    - Captures only relevant events.
        
- **Limitations**:
    
    - Harder to apply certain statistical models (may need resampling).
        

---

### **1.6 Graph Data**
![[Graph Data.png]]
- **Definition**:  
    Data stored as nodes (entities) and edges (relationships).
    
- **Examples**:
    
    - Social media networks (user-to-user connections).
        
    - Fraud detection (transactions and their relationships).
        
- **Advantages**:
    
    - Efficient for relationship-based queries.
        
    - Good for traversing connected data.
        
- **Limitations**:
    
    - Requires specialized databases (Neo4j, OrientDB).
        
- **Applications**:
    
    - Recommendation engines, supply chain mapping.
        

---

### **1.7 Multimedia Data**
![[Multimedia Data.png]]
- **Definition**:  
    Data in image, video, audio, or graphics format.
    
- **Examples**:
    
    - Digital photos, surveillance videos, audio recordings.
        
- **Advantages**:
    
    - Highly engaging and rich in information.
        
- **Limitations**:
    
    - Large file sizes require high storage and processing capacity.
        
- **Applications**:
    
    - Media streaming, advertising, facial recognition, voice analytics.
## 2. Data Architecture 
### 2.1 Concepts
Data Architecture is theÂ **â€œoverall blueprintâ€**Â for the entire lifecycle of data within an organization.  
It specifiesÂ **where the data comes from, where it is stored, how it is processed, and how it is ultimately used**.

If we think of data as raw materials, then Data Architecture is like a factory assembly line that ensures the raw materials go through the right paths, are processed correctly, and result in high-quality finished products.

---
### 2.2 Role
- Connects data from various sources into aÂ **unified system**.
    
- EnsuresÂ **high-quality data**Â â€“ accurate, complete, consistent, and ready for use.
    
- Supports decision-making based onÂ **processed and analyzed data**.
    
- Reduces cost and risk byÂ **optimizing data flows**Â and implementingÂ **centralized governance**
### 2.3 Main Layers in Data Architecture

#### 2.3.1 Data Sources (Nguá»“n dá»¯ liá»‡u)

This is whereÂ **all the raw data originates**.  
Think of it as the â€œinput warehouseâ€ of your factory.

- **Transactional Databases**Â â€“ Data from applications that handle day-to-day transactions (e.g., sales systems, banking systems).
    
- **Log Files**Â â€“ Records of system activity, user actions, and error tracking.
    
- **Applications**Â â€“ Business software like CRM, ERP that generate data.
    
- **IoT Sensors**Â â€“ Devices that collect environmental readings like temperature, location, machine status.
    
- **APIs**Â â€“ Data pulled from other systems or platforms via programmatic access.
    
- **Third-party Data**Â â€“ Data purchased or received from external providers, such as market research or demographic data.
    

ğŸ’¡Â _In short: This is the â€œraw ingredientâ€ stage before any cleaning or processing happens._

---

#### 2.3.2 Operational Data Layer (Lá»›p dá»¯ liá»‡u váº­n hÃ nh)

TheÂ **working area**Â where data is stored and accessed forÂ **real-time business operations**.

- Usually based onÂ **OLTP (Online Transaction Processing)**Â systems.
    
- Handles frequent updates, inserts, and queries.
    
- Focuses onÂ **speed and accuracy**Â to support day-to-day activities like order entry, banking transactions, or inventory updates.
    

ğŸ’¡Â _Analogy: This is like the â€œproduction floorâ€ where materials are actively used in daily operations._

---

#### 2.3.3 Ingestion & Staging Layer (Lá»›p tiáº¿p nháº­n vÃ  lÆ°u táº¡m)

AÂ **temporary holding area**Â for raw data before it is processed.

- **Data Ingestion**: Pulling in data from multiple sources.
    
- **Staging**: Storing that data in its raw form without altering it.
    
- Allows data engineers toÂ **validate quality**Â before it moves forward.
    
- ProtectsÂ **original, unmodified copies**Â for backup and auditing.
    

ğŸ’¡Â _Analogy: This is the â€œreceiving dockâ€ where goods arrive and are checked before being sent into production._

---

#### 2.3.4 Integration & Transformation Layer (Lá»›p tÃ­ch há»£p vÃ  chuyá»ƒn Ä‘á»•i)

This is whereÂ **data cleaning, merging, and formatting**Â happens.

- **Integration**: Combine data from multiple sources into one consistent format.
    
- **Transformation**: Change data structure, remove duplicates, standardize units, fix missing values.
    
- Ensures data isÂ **accurate, consistent, and analysis-ready**.
    

ğŸ’¡Â _Analogy: This is the â€œquality control & assembly areaâ€ where raw parts are processed into usable components._

---

#### 2.3.5 Analytics Data Layer (Lá»›p phÃ¢n tÃ­ch dá»¯ liá»‡u)

AÂ **centralized storage**Â for cleaned and structured data ready for reporting and analysis.

- Often implemented asÂ **Data Warehouse**Â orÂ **Data Mart**.
    
- StoresÂ **historical data**Â for trend analysis and strategic planning.
    
- SupportsÂ **OLAP (Online Analytical Processing)**Â for deep analysis and querying.
    

ğŸ’¡Â _Analogy: This is the â€œfinished goods warehouseâ€ where products are ready to be shipped to customers (in this case, the customers are analysts and decision-makers)._

---

#### 2.3.6 Business Intelligence (BI) Layer (Lá»›p thÃ´ng tin kinh doanh)

TheÂ **user-facing layer**Â where decision-makers interact with the data.

- ProvidesÂ **dashboards**,Â **reports**, andÂ **data visualizations**.
    
- Tools likeÂ **Power BI, Tableau, Qlik**Â help managers and staff extract insights quickly.
    
- SupportsÂ **data-driven decision making**Â at all levels of the organization.
    

ğŸ’¡Â _Analogy: This is the â€œstorefrontâ€ where the final products are displayed and delivered to customers in the most usable form._

### 2.4 Components of Data Architecture
#### 2.4.1 Data Modelling

##### 2.4.1.1 Definition
Data Modelling is the process ofÂ **identifying, organising, and representing the structure of data**Â in a system so that it is clear, consistent, and understandable for both technical teams and business stakeholders.

- It definesÂ **what data needs to be stored**,Â **how it relates to other data**, andÂ **what business rules**Â apply.
    
- Think of it as the â€œblueprintâ€ of the data system, similar to how an architectural drawing works for a building.
    

---

##### 2.4.1.2 Main Objectives

- EnsureÂ **data consistency**Â across the entire system.
    
- Provide aÂ **common language**Â for everyone involved in the project.
    
- Reduce errors during implementation by creating a complete, agreed-upon plan before coding.
    
- Create a foundation for query optimisation and data management.
    

---

##### 2.4.1.3 Three Levels of Data Modelling

###### a) Conceptual Data Model
![[Conceptual Data Model.png]]
- **Purpose:**Â Provide a big-picture view of the data in the system.
    
- **Focus:**
    
    - IdentifyÂ **entities**Â (things we store information about) andÂ **relationships**Â between them.
        
    - DoesÂ **not**Â define tables or database structures yet.
        
- **Characteristics:**
    
    - High-level, business-oriented.
        
    - Example: â€œCustomerâ€ places â€œOrdersâ€.
        
    - May show relationships such as one-to-many or many-to-many, but not the exact table design.
        
- **When used:**Â Early in the project, to align understanding with stakeholders.
    

---
###### b) Logical Data Model
![[Logical Data Model.png]]
- **Purpose:**Â Turn the conceptual model into a more technical structure, but stillÂ **not tied to any specific technology**.
    
- **Focus:**
    
    - DefineÂ **attributes**Â for each entity.
        
    - SpecifyÂ **primary keys (PK)**Â andÂ **foreign keys (FK)**Â logically.
        
    - Show relationship cardinality (1:1, 1:N, M:N).
        
- **Characteristics:**
    
    - More detail than conceptual, but independent of the database platform.
        
    - Example: â€œCustomerâ€ table hasÂ `CustomerID`,Â `Name`,Â `Email`; â€œOrderâ€ table hasÂ `OrderID`,Â `CustomerID`,Â `OrderDate`.
        
- **When used:**Â After finalising the conceptual model, to prepare for database design.
    

---

###### c) Physical Data Model
![[Physical Data Model'.png]]
- **Purpose:**Â Implement the logical model into anÂ **actual database structure**Â on a specific DBMS.
    
- **Focus:**
    
    - CreateÂ **real database tables**.
        
    - DefineÂ **exact data types**Â (`VARCHAR`,Â `INT`,Â `DATE`, etc.).
        
    - AddÂ **indexes, constraints, partitions**Â for performance optimisation.
        
    - Set storage, backup, and user permissions.
        
- **Characteristics:**
    
    - Highly technical, depends on chosen technology (e.g., MySQL, PostgreSQL, Oracle).
        
    - Example:
        
        sql
        
        CopyEdit
        
        `CREATE TABLE Customers (   CustomerID INT PRIMARY KEY,   Name VARCHAR(100),   Email VARCHAR(100) );`
        
- **When used:**Â During implementation, before starting application development.
    

---

##### 2.4.1.4 Relationship Between the Three Levels

- **Conceptual:**Â DecidesÂ **WHAT**Â to store.
    
- **Logical:**Â DecidesÂ **HOW**Â to organise data logically.
    
- **Physical:**Â DecidesÂ **HOW**Â to implement it technically in the database.
    

---

##### 2.4.1.5 Benefits of Data Modelling

- Reduces misunderstandings between business and technical teams.
    
- Makes maintenance and future expansion easier.
    
- Enables accurate reporting and analytics.
    
- Improves system performance by optimising design early.
#### 2.4.2 Data Ingestion 
##### 2.4.2.1 Definition 
Data Ingestion isÂ **the process of collecting and bringing data from multiple sources**Â into a centralized storage location (Data Destination) such as a Data Lake, Data Warehouse, or analytics platform for storage, processing, and analysis.

- It is theÂ **first step**Â in a data pipeline.
    
- Can happen inÂ **batch mode**Â orÂ **real-time/streaming**Â mode.
##### 2.4.2.2 Purpose

- BringÂ **data from various sources**Â (databases, IoT devices, APIs, applications, external providers) into a centralised system.
    
- Ensure data isÂ **captured in a consistent, accurate, and timely manner**.
    
- SupportÂ **real-time decision-making**Â orÂ **batch processing**Â for reports and analytics.
    
---

##### 2.4.2.3 Types of Data Ingestion

1. **Batch Ingestion**
    
    - Data is collected and loaded atÂ **scheduled intervals**Â (e.g., hourly, daily).
        
    - Best for large volumes of data that donâ€™t require instant updates.
        
    - Example: Loading daily sales transactions from a storeâ€™s POS system to a central data warehouse at midnight.
        
2. **Real-Time (Streaming) Ingestion**
    
    - Data is ingestedÂ **continuously**Â as it is created or updated.
        
    - Useful for applications that requireÂ **instant insights**.
        
    - Example: Live stock price updates, IoT sensor readings, or social media feeds.
        
3. **Micro-Batch Ingestion**
    
    - Hybrid approach: very small batches processed frequently (e.g., every few minutes).
        
    - Balances speed with processing efficiency.
#####  2.4.2.4 General Workflow
Data ingestion typically follows one of two main patterns:Â **ETL**Â orÂ **ELT**
###### a)Â ETL (Extract â€“ Transform â€“ Load)
**Definition of ETL (Extract â€“ Transform â€“ Load)**  
	ETL is a traditional data processing model, especially common for loading data into anÂ **Enterprise Data Warehouse (EDW)**.  
	The process consists of three main steps:
	- **Extract**
		- Copy data from various source systems into aÂ **staging area**.
		- Usually performed inÂ **batch mode**Â (e.g., every hour, every day).
	- **Transform**
		- **Data cleansing**Â â†’ remove errors, duplicates, and missing values.		
		- **Data enrichment**Â â†’ add necessary supplementary information.
		- Convert structure and format so the data fits theÂ **data warehouse schema**.
	- **Load**
		- Load the processed data into the tables of theÂ **data warehouse**.
		
---

**Limitations of the ETL Model**
- Suitable only forÂ **batch processing**Â â†’ not optimized forÂ **real-time data processing**.

- Rigid schema in EDW â†’ difficult to change or adapt when the source data structure changes.
    
- Does not supportÂ **unstructured data**Â â†’ data without a schema (unstructured) or binary data is difficult to load into EDW.
    
- Loss of data that cannot be loaded â†’ if data canâ€™t be loaded into EDW, it becomes unavailable for analysis.
    
- Loss ofÂ **lineage information**Â (data origin) â†’ once loaded into EDW, the source history is not retained, making backtracking difficult.
    
---
**When to Use ETL?**

- When data isÂ **stable**Â and schema changes are rare.
    
- WhenÂ **real-time analytics**Â is not required.
    
- When strictÂ **data control and standardization**Â is needed before storage.
######  b)Â ELT (Extract â€“ Load â€“ Transform)
**Definition of ELT**  
ELT is a variation of ETL, but the processing order changes: instead ofÂ **Extract â†’ Transform â†’ Load**, ELT performsÂ **Extract â†’ Load â†’ Transform**.

**Steps in ELT:**

- **Extract and Load**
    
    - Data from multiple source systems is extracted and directly loaded into aÂ **centralized repository**, often in the form of aÂ **data lake**.
        
    - The data is stored in itsÂ **raw form**Â immediately upon loading, without any processing.
        
- **Transform**
    
    - When analysis is needed, the data in the data lake isÂ **cleansed**Â (error removal, handling missing values),Â **enriched**Â (adding supplemental data), andÂ **transformed**Â (structural and format changes).
        
    - The processed data is then sent to the most appropriate location for analysis, for example:
        
        - **Data Warehouse**Â (with clear tabular structure)
            
        - **NoSQL Database**Â (for unstructured data)
            

---

 **Advantages of ELT**

- **Preserves Raw Data**Â â€“ Keeps all original data in the data lake, protecting historical records andÂ **data lineage**(origin tracking).
    
- **Supports All Data Types**Â â€“ Works with structured, semi-structured (JSON, XML), unstructured (text, images, videos), and binary data.
    
- **Flexible**Â â€“ You can transform data only when needed, rather than before loading.
    
- **Faster Loading**Â â€“ Since transformation is skipped during ingestion, large datasets can be loaded quickly.
    
- **Real-time Friendly**Â â€“ Works well with streaming and real-time ingestion.
    

---

 **When to Use ELT**

- When you haveÂ **large volumes**Â of data from many sources.
    
- When you need toÂ **retain raw historical data**Â for auditing, compliance, or future analysis.
    
- When your analytics environment needs toÂ **handle diverse data types**Â (structured + unstructured).
    
- When you useÂ **modern cloud-based platforms**Â that can store and process raw data at scale (e.g., Snowflake, Databricks, BigQuery).
###### c) ETL vs ELT 
 **1. Processing Order**
- **ETL**: Extract â†’Â **Transform**Â â†’ Load
    
    - Data is cleaned and transformedÂ **before**Â loading into the target system.
        
- **ELT**: Extract â†’Â **Load**Â â†’ Transform
    
    - Data is loadedÂ **first**Â in its raw form, then transformedÂ **later**Â when needed.
        

---
 **2. Where the Transformation Happens**

- **ETL**: In aÂ **separate ETL processing engine**Â before the data reaches the data warehouse.
    
- **ELT**: Inside theÂ **target system**Â (data lake, cloud warehouse) after data is already stored.
    

---
 **3. Storage of Raw Data**

- **ETL**: Raw data isÂ **not stored**Â â€” only processed data is kept in the data warehouse.
    
- **ELT**: Raw data isÂ **stored and preserved**, which means you keep full historical records and lineage.
    

---
 **4. Data Types Supported**

- **ETL**: MostlyÂ **structured data**Â (tabular, fixed schema).
    
- **ELT**: Can handleÂ **structured, semi-structured, unstructured, and binary**Â data.
    

---
 **5. Speed and Flexibility**

- **ETL**: Slower to load large datasets because transformation happens first.
    
- **ELT**: Faster to load because transformation is skipped during ingestion, and more flexible since transformation can be done anytime.
    

---
 **6. Real-Time Capability**

- **ETL**: Mainly batch processing, not optimized for real-time.
    
- **ELT**: Works well withÂ **real-time and streaming**Â data ingestion.
##### 2.4.2.5 Data Sources
Common types include:

- **Apache Kafka**Â (streaming platform)
    
- **JDBC**Â (database connectors)
    
- **Oracle CDC**Â (Change Data Capture)
    
- **HTTP Clients**Â (API data ingestion)
    
- **HDFS**Â (Hadoop Distributed File System)
    

---

##### 2.4.2.5 Data Destinations  
Common destinations include:

- **Apache Kafka**Â (for streaming pipelines)
    
- **JDBC**Â (to write into databases)
    
- **Snowflake**Â (cloud data warehouse)
    
- **Amazon S3**Â (cloud object storage)
    
- **Databricks**Â (big data analytics platform)
#### 2.4.3 Data Management Systems
##### 2.4.3.1 Definition
AÂ **Data Management System**Â is a platform or set of tools that allows an organization toÂ **store, manage, process, and retrieve data**Â efficiently.  
Its goal is to ensure that data isÂ **accurate, complete, secure, easily accessible**, andÂ **ready to support analysis and decision-making**.

---
##### 2.4.3.2 Main Components

###### a)Â Data Warehouse
1.Â **Definition**
AÂ **Data Warehouse (DW)**Â is aÂ **large-scale, centralized storage system**Â designed toÂ **collect, store, and manage data**from multiple sources for the purpose ofÂ **analysis, reporting, and business intelligence (BI)**.  
Unlike operational databases, which are optimized for day-to-day transaction processing (OLTP), a DW is optimized forÂ **querying large volumes of historical data**Â (OLAP â€“ Online Analytical Processing).

---

2.Â **Purpose**

- **Integrate data**Â from multiple systems (ERP, CRM, sales databases, sensors, etc.) into a single, unified format.
    
- **Support decision-making**Â by providing accurate, consistent, and up-to-date analytical data.
    
- AllowÂ **fast, complex queries**Â without affecting operational systems.
    
- Provide aÂ **"single source of truth"**Â for the organizationâ€™s historical and analytical data.
    

---

 3.Â **Key Characteristics**

- **Structured Data Only**Â â†’ Data must be cleaned, validated, and stored in a defined schema before loading.
    
- **Read-Optimized**Â â†’ Designed for high-performance queries and analytics, not for frequent writes or updates.
    
- **Historical Focus**Â â†’ Stores years of historical data for trend analysis and forecasting.
    
- **Centralized Repository**Â â†’ All analytical data is stored in one place to ensure consistency.
    

---

 4.Â **Three-Tier Data Warehouse Architecture**

 **Tier 1 â€“ Bottom Tier (Data Source Layer)**

- Data comes fromÂ **internal systems**Â (transaction databases, log files, etc.) andÂ **external sources**Â (third-party data providers).
    
- **ETL process**Â (Extract â†’ Transform â†’ Load) cleans, standardizes, and prepares data.
    
- Data is stored in theÂ **staging area**Â before being loaded into the warehouse.
    

 **Tier 2 â€“ Middle Tier (Data Storage Layer)**

- **Central database**Â for storing processed, integrated data.
    
- Managed by aÂ **Database Server**Â (often an OLAP server).
    
- Data is organized intoÂ **fact tables**Â (quantitative metrics) andÂ **dimension tables**Â (descriptive attributes).
    

 **Tier 3 â€“ Top Tier (Presentation Layer)**

- **Business Intelligence & Analytics tools**Â connect to the DW to perform:
    
    - Data mining
        
    - Reporting
        
    - Dashboards
        
    - Trend and forecasting analysis
        
- Examples: Tableau, Power BI, Qlik.
    

---

 5.Â **Advantages**

- **High Performance Queries**Â â†’ Optimized indexing and schema for fast analytics.
    
- **Data Consistency**Â â†’ All data is standardized into one schema.
    
- **Business Intelligence Support**Â â†’ Enables complex analysis, KPIs, and strategic insights.
    
- **Historical Analysis**Â â†’ Long-term trend identification and forecasting.
    

---

 6.Â **Limitations**

- **Schema rigidity**Â â†’ Changes in business requirements or source systems may require redesigning the schema.
    
- **No unstructured data support**Â â†’ Only works well with structured, schema-defined data.
    
- **High cost**Â â†’ Requires significant infrastructure and maintenance.
    
- **Not Real-Time**Â â†’ Usually batch-loaded, so it may not reflect the very latest transactions.
    

---

 7.Â **When to Use a Data Warehouse**

- When you needÂ **organization-wide reporting**.
    
- When your data sources are mostlyÂ **structured**Â and relatively stable in schema.
    
- When historical analysis and trend forecasting are important.
    
- When you want aÂ **single, consistent version of the truth**Â for decision-making.
---

###### b)Â Data Mart
 1.Â **Definition**
AÂ **Data Mart**Â is aÂ **subset**Â of aÂ **Data Warehouse**, designed to serve the needs of aÂ **specific group of users, department, or business function**Â within an organization.

If aÂ **Data Warehouse**Â is the â€œmain warehouseâ€ containing all processed enterprise data, aÂ **Data Mart**Â is like a â€œsmaller store sectionâ€ that contains only the data relevant toÂ **one specific purpose**.

**Examples:**

- Marketing department â†’Â **Marketing Data Mart**Â with campaign, lead, and market analysis data.
    
- Finance department â†’Â **Finance Data Mart**Â with revenue, cost, and profit data.
    

---

 2.Â **Purpose**
- ProvideÂ **quick access**Â to data relevant to a specific business function.
    
- Reduce complexity compared to a full Data Warehouse (DW) â†’ smaller and easier to use.
    
- **Improve query performance**Â because it only contains necessary data for that user group.
    
- Reduce data processing cost and time.
    

---

 3.Â **Types of Data Marts**

	 **Dependent Data Mart**
	- BuiltÂ **from an existing Data Warehouse**.
	    
	- Uses aÂ **top-down approach**: data is loaded into the DW first, then a subset is extracted to create the Data Mart.
	    
	- **Pros:**
	    
	    - Data is consistent with the DW.
	        
	    - Centralized management and easier maintenance.
	        
	- **Cons:**
	    
	    - Fully dependent on the DW â€” if DW data is delayed, the Data Mart is also delayed.
	        
	
	---
	
	**Independent Data Mart**
	
	- OperatesÂ **independently**Â of a Data Warehouse.
	    
	- Sources data directly from operational or external systems â†’ processes it â†’ stores it in the Data Mart.
	    
	- **Pros:**
	    
	    - Quick deployment, no need for a DW first.
	        
	    - Good for analyzing a small, focused dataset.
	        
	- **Cons:**
	    
	    - **Data inconsistency**Â risk if multiple independent marts exist.
	        
	    - Harder to integrate into a centralized DW later.
	        
	
	---
	
	**Hybrid Data Mart**
	
	- Combines data from bothÂ **a Data Warehouse**Â andÂ **other operational sources**.
	    
	- More flexible â€” has the speed of an independent mart plus the consistency of a dependent mart.
	    
	- Common when:
	    
	    - Real-time operational data is needed.
	        
	    - Historical data from DW is still valuable.
	        

---

 4.Â **Advantages**

- **Faster**Â â†’ quicker deployment and query times than a full DW.
    
- **Simpler**Â â†’ only contains relevant data.
    
- **Lower cost**Â compared to building a full DW for one department.
    
- **Focused**Â â†’ tailored to the needs of a specific group.
    

---

 5.Â **Disadvantages**

- Without proper synchronization,Â **data discrepancies**Â may arise between Data Marts and DW.
    
- **Limited analysis scope**Â â€” only department-specific data.
    
- Independent marts can causeÂ **data duplication**Â and waste resources.
    

---

 6.Â **When to Use**

- When a department needs fast analyticsÂ **without waiting for DW implementation**.
    
- When analysis isÂ **focused on a specific dataset or function**.
    
- When improving query performance is a priority due to smaller data volume.

---
###### c)Â Data Lake
 1.Â **Definition**

AÂ **Data Lake**Â is aÂ **centralized storage repository**Â that holdsÂ **large volumes of raw data**Â in itsÂ **native format**Â â€” whetherÂ **structured, semi-structured, or unstructured**Â â€” without requiring transformation before storage.

It is built forÂ **scalability, flexibility, and cost efficiency**, making it ideal forÂ **big data, advanced analytics, artificial intelligence (AI), and machine learning (ML)**Â workloads.

ğŸ“Œ Analogy: If aÂ **Data Warehouse**Â is a neatly organized supermarket, aÂ **Data Lake**Â is like a massive storage yard where you keep everything first, then organize and process it only when needed.

---

 2.Â **Purpose**

- StoreÂ **all types of data**Â from multiple sources in one place.
    
- EnableÂ **schema-on-read**Â â†’ apply structure only when the data is read for analysis.
    
- Support modern analytics, AI, and ML that require raw, diverse datasets.
    
- Reduce preparation time for data scientists and analysts.
    

---

 3.Â **Key Characteristics**

- **Stores raw data**Â exactly as received, without forcing it into a fixed schema.
    
- **Highly scalable**Â â€” can hold petabytes (PB) or even exabytes (EB) of data.
    
- **Cost-effective storage**Â using cloud-based object storage systems.
    
- **Supports all data types**:
    
    - **Structured**Â (tables, CSV files, relational DB dumps)
        
    - **Semi-structured**Â (JSON, XML, logs)
        
    - **Unstructured**Â (text, images, videos, audio)
        
    - **Binary**Â files
        
- **Compatible with multiple processing frameworks**Â (batch, streaming, real-time).
    

---

 4.Â **Typical Architecture**

 **a) Ingestion Layer**

- Collects data from various sources: databases, APIs, streaming platforms (e.g., Kafka), IoT devices, files, etc.
    
- UsesÂ **ELT**Â or streaming pipelines to ingest data into the lake.
    

 **b) Storage Layer**

- Stores raw data in cloud object storage (AWS S3, Azure Data Lake Storage, Google Cloud Storage).
    
- Often partitioned by source, date, or metadata for easier retrieval.
    

 **c) Processing Layer**

- Processes dataÂ **on demand**Â using tools like Apache Spark, Databricks, Presto, Flink.
    
- SupportsÂ **batch**Â andÂ **real-time**Â processing.
    

**d) Consumption Layer**

- Serves data to BI tools, analytics platforms, AI, and ML systems.
    
- Examples: Tableau, Power BI, Jupyter Notebook, TensorFlow, PyTorch.
    

---

 5.Â **Advantages**

- **Stores everything**Â â€” no need to decide upfront what to keep.
    
- **Flexible**Â â€” works with any data format.
    
- **Scalable**Â â€” can grow without performance loss.
    
- **Cost-efficient**Â â€” cloud object storage is cheaper than relational databases.
    
- **Supports modern analytics**Â â€” AI, ML, predictive analytics, and big data processing.
    

---

6.Â **Challenges**

- **Data swamp risk**Â â€” without proper governance, the lake can become unorganized and hard to use.
    
- **Security**Â â€” sensitive data must be encrypted and access-controlled.
    
- **Metadata management**Â â€” without proper cataloging, finding and understanding data becomes difficult.
    
- **Skilled workforce needed**Â â€” requires data engineers and scientists with advanced tools.
    

---

 7.Â **When to Use a Data Lake**

- When data comes inÂ **various formats**Â (structured, semi-structured, unstructured).
    
- When you need to retainÂ **raw historical data**Â for compliance, auditing, or future analysis.
    
- When working onÂ **machine learning, AI, or big data analytics**.
    
- When you needÂ **real-time streaming**Â andÂ **batch processing**Â together.
#### 2.4.4 Type of Data Architecture
##### 2.4.4.1 Data Fabric
 **1. Definition**
**Data Fabric**Â is a data architecture that enablesÂ **end-to-end integration**Â of data from multiple sources (databases, data warehouses, data lakes, APIs, cloud systems, and external platforms) into aÂ **unified, intelligent, and automated ecosystem**.
- It creates aÂ **virtualized, metadata-driven layer**Â on top of existing data systems.
    
- Users and applications canÂ **access data without knowing its physical location, format, or storage details**.
    

> In simple terms:Â **Data Fabric is a â€œdata network fabricâ€ that connects and integrates all data across the organization seamlessly**.

---

 **2. Components & How It Works**
From the slides, we can view Data Fabricâ€™s evolution inÂ **three stages**:

 **a. Traditional Data Management**
![[Traditional Data Management.png]]
- Data from multiple sources (**Operational Systems, Individual Databases, Websites, Social Media, IoT Devices, Flat Files**) is collected throughÂ **ETL**Â and stored inÂ **Data Warehouses, Data Lakes, or Data Marts**.
    
- Consumers (BI tools, reporting systems, applications) access the data viaÂ **APIs**Â orÂ **SQL**.
    
- **Limitation:**Â Data must be moved and duplicated to central storage before use â†’ slower and more resource-intensive.
    

---

 **b. Data Virtualization Architecture**
![[Data Virtualization Architecture.png]]
- Introduces anÂ **Abstraction Layer**Â containing:
    
    - **Virtual View**: Provides a combined view of data from multiple sources without physically moving it.
        
    - **Data Catalog & Metadata**: Stores information about data location, structure, meaning, and relationships.
        
- Allows direct access via APIs or SQL without heavy data replication.
    

---

 **c. Data Fabric (Modern Approach)**
 ![[Data Fabric (Modern Approach).png]]
- Builds onÂ **Data Virtualization**Â but addsÂ **AI/ML automation**Â andÂ **metadata-driven orchestration**:
    
    - **Knowledge Graph**: Maps and visualizes relationships between datasets.
        
    - **Recommendation Engine**: Suggests relevant datasets for analysis.
        
    - **Business Rules**: Automates data governance and processing logic.
        
    - **Metadata & Data Catalog**: Centralized data descriptions to track lineage and improve discovery.
        
- AI/ML helps automate:
    
    - **Ingestion**: Loading data from diverse sources.
        
    - **Transformation**: Cleaning and enriching data.
        
    - **Data Delivery**: Supplying data to BI tools, dashboards, and business apps.
        

---

**3. Key Characteristics**Â 

- **Data Integration**: Connects all types of sourcesâ€”databases, warehouses, lakes, APIs, and external systems.
    
- **Data Virtualization**: Lets applications access data without needing to know where it is physically stored.
    
- **Data Governance**: Implements security, privacy, compliance, and quality controls.
    
- **Data Orchestration**: Automates pipelines, workflows, and delivery processes.
    
- **Metadata Management**: Maintains a centralized metadata repository for complete data asset visibility (definitions, relationships, lineage).
    

---

**4. Benefits**

- **Real-time access**Â to distributed data without duplication.
    
- **Keeps original data at the source**, reducing redundancy.
    
- Speeds up integration and analysis.
    
- Flexible to add new sources without heavy reengineering.
    
- SupportsÂ **structured, semi-structured, unstructured, and streaming**Â data.
    
---

**5. Best Use Cases**

- Organizations withÂ **multi-cloud, hybrid, or on-premise**Â data environments.
    
- Businesses that needÂ **fast analytics**Â without waiting for ETL processes.
    
- Scenarios whereÂ **data must remain in source systems**Â for compliance or cost reasons.
    
- Enterprises that wantÂ **metadata-driven insights**Â andÂ **AI-assisted data discovery**.
##### 2.4.4.2 Data Mesh
 **1. Definition**

**Data Mesh**Â is aÂ **decentralized data architecture**Â that focuses on delegating data management responsibilities to each business domain (e.g., Marketing, Sales, Finance) and treats data as a product (**Data as a Product**).

- Each domainÂ **owns, manages, and is responsible**Â for its own data.
    
- Domains create, store, process, and share their data through aÂ **self-serve data infrastructure**.
    
- AÂ **federated governance**Â framework ensures that data from different domains is interoperable, secure, and reliable.
    

> In other words:Â **Data Mesh turns data into a â€œproductâ€ and gives each team full responsibility for the entire lifecycle of that data**.

---

 **2. Components & How It Works**
  **a. Traditional Centralized Data Management**

- A central data team collects, processes, and manages data from all domains.
    
- Data is stored in a centralizedÂ **Data Warehouse**Â orÂ **Data Lake**.
    
- **Limitation:**Â Prone to bottlenecks, lacks deep domain knowledge, and responds slowly to specialized data needs.
- 
- **b. Data mesh Approach**
 ![[Data Mesh.png]]
- **Domain-Oriented Ownership:**Â Each domain owns and manages its own data.
    
- **Data as a Product:**
    
    - Data comes with clear metadata, APIs, and documentation.
        
    - Standards for quality, security, and accessibility are applied.
        
- **Self-Serve Data Platform:**
    
    - A shared platform provides tools for domains to ingest, transform, store, and share data without depending on the central team.
        
- **Federated Computational Governance:**
    
    - A unified set of rules ensures that all domain data meets organizational standards.
        
    - Governance is automated via infrastructure.
        

---

 **3. Key Characteristics**

- **Decentralized Data Ownership:**Â Data responsibility lies within the domains.
    
- **Data Product Mindset:**Â Data is treated as a product, serving multiple consumers.
    
- **Self-Service Infrastructure:**Â Domains have full access and tools to work with their data.
    
- **Federated Governance:**Â Combines common standards with domain autonomy.
    
- **Interoperability:**Â Data from different domains can be easily integrated.
    

---

 **4. Benefits**

- **Scalable:**Â Easily expand without overloading the central data team.
    
- **Faster Delivery:**Â Domains can quickly create and share data.
    
- **Higher Quality:**Â Data is created and managed by those with deep domain knowledge.
    
- **Flexibility:**Â Each domain can choose the tools and methods that suit them best.
    

---

 **5. Best Use Cases**

- Organizations with multiple independent domains and large data volumes.
    
- Businesses needing agility and speed in creating data products.
    
- Complex and diverse data requiring deep domain expertise.
    
- When wanting to avoid bottlenecks caused by a centralized data team.
    

---
## 3. Big Data Era
### 3.1Â **Definition**

TheÂ **Big Data Era**Â describes the current stage of computing and data management where:

- TheÂ **volume**Â of data is massive (terabytes to petabytes).
    
- TheÂ **velocity**Â is high (data arrives in real-time or streams).
    
- TheÂ **variety**Â of data is large (structured, semi-structured, unstructured, multimedia).
    
- TheÂ **veracity**Â of dataâ€”its quality and reliabilityâ€”must be maintained despite scale.
    
- TheÂ **value**Â comes from using this data to gain insights, make predictions, and drive business actions.
### 3.2Â **Shift in Data Management Requirements**

#### **Legacy Demands**Â (Old Approach)

- Small number of users.
    
- Designed to run on aÂ **single server**.
    
- Focused onÂ **transactions**Â with ACID properties, consistency, and integrity.
    
- Goal wasÂ **storage efficiency**Â and optimized relational database structures.
    

#### **Modern Demands**Â (Big Data Era)

- Massive numbers ofÂ **users and devices**.
    
- NeedÂ **scale and performance**Â to handle large datasets quickly.
    
- **Flexibility**Â for agile development and fast adaptation.
    
- Adoption ofÂ **microservices architectures**.
    
- Support forÂ **web, mobile, and IoT**Â experiences.
### 3.3Â ***Architectural Shifts in Big Data Era***
### **1. Generation**

- **From (Before):**
    
    - MostlyÂ **on-prem(physically inside the organizationâ€™s own facilitiesÂ â€” instead of being hosted in the cloud)** relational databases.
        
    - OnlyÂ **structured data**.
        
    - Data collected inÂ **batch**Â mode.
        
- **To (Now):**
    
    - **Cloud-based relational**Â andÂ **NoSQL databases**.
        
    - BothÂ **structured**Â andÂ **unstructured**Â data.
        
    - HandlesÂ **real-time**Â and batch.
        
- **Rationale:**
    
    - Moving to theÂ **cloud**Â forÂ **flexibility**Â andÂ **scalability**.
        
    - **Polyglot persistence**: use the most suitable database type for each task.
        

---

### **2. Ingestion**

- **From:**
    
    - **On-prem ETL tools**.
        
    - Structured data only.
        
    - Batch mode.
        
- **To:**
    
    - **Cloud-based ELT tools**.
        
    - Structured + unstructured data.
        
    - Real-time & batch processing.
        
- **Rationale:**
    
    - TraditionalÂ **ETL**Â replaced by moreÂ **flexible**Â andÂ **high-performance ELT**, supporting real-time ingestion.
        

---

### **3. Storage**

- **From:**
    
    - **On-prem centralized data warehouse**.
        
- **To:**
    
    - **Cloud-based data warehouse**.
        
    - **Distributed storage**Â systems.
        
    - **Data lakes**Â for raw data in multiple formats.
        
- **Rationale:**
    
    - GreaterÂ **flexibility**Â in collecting and storing from multiple sources,Â **scalability**, andÂ **advanced analytics**.
        

---

### **4. Analytics**

- **From:**
    
    - **Historical**Â andÂ **prescriptive**Â analytics (recommendations based on past data).
        
    - Batch processing.
        
    - Traditional data mining.
        
- **To:**
    
    - **Prescriptive & predictive**Â analytics (forecasting future trends).
        
    - Real-time and batch.
        
    - **AI/ML models**.
        
- **Rationale:**
    
    - Moving from justÂ **prescriptive**Â toÂ **predictive**Â analytics.
        
    - LeveragingÂ **AI/ML approaches**.
        

---

### **5. Consumption**

- **From:**
    
    - **Dashboards & reports**.
        
    - Centralized administration.
        
- **To:**
    
    - **AI/ML model-driven insights**.
        
    - **Self-service**Â analytics for business users.
        
- **Rationale:**
    
    - EnablesÂ **non-technical users**Â to run analytics themselves via self-service portals.
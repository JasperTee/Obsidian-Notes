## 1. Types of Data

### **1.1 Structured Data**
![[Structured Data.png]]
- **Definition**:  
    Data stored in aÂ **predefined tabular format**Â with rows and columns, typically inÂ **relational databases**.
    
- **Characteristics**:
    
    - Each row is a record, each column holds a specific attribute.
        
    - Schema (structure) is fixed.
        
    - Querying is done withÂ **SQL**.
        
- **Lecture Example**:  
    Table of journal articles: Title, Journal, Volume, Issue, Date, Pages.
    
- **Advantages**:
    
    - High accuracy and consistency.
        
    - Easy to maintain, backup, and integrate.
        
    - Mature tools and strong query support.
        
- **Limitations**:
    
    - Rigid structure, hard to handle rapidly changing data formats.
        
- **Applications**:
    
    - Banking transactions, retail sales data, employee records.
        

---

### **1.2 Semi-structured Data**
![[Semi-structured data.png]]
- **Definition**:  
    Data that doesnâ€™t fit perfectly into tables but hasÂ **tags, markers, or keys**Â that make it self-describing.
    
- **Common Formats**: XML, JSON, YAML.
    
- **Characteristics**:
    
    - Flexible schema, not all records must have the same attributes.
        
    - Self-describing â€” each field has its own label.
- **Advantages**:
    
    - Easier to evolve as needs change.
        
    - Works well for data exchange between systems.
        
- **Limitations**:
    
    - Slower queries compared to structured data.
        
    - Needs parsing and validation for consistency.
        
- **Applications**:
    
    - API responses, system configurations, cross-platform data exchange.
        

---

### **1.3 Unstructured Data**
![[Unstructured Data.png]]
- **Definition**:  
    Data without a predefined data model or organization.
    
- **Examples**:
    
    - Emails, PDF files, Word documents.
        
    - Social media posts, chat messages.
        
    - Raw server logs.
        
- **Advantages**:
    
    - Can store any content type.
        
    - Rich in detail and context.
        
- **Limitations**:
    
    - Hard to search and analyze directly.
        
    - Often requires AI, NLP, or data preprocessing.
        
- **Applications**:
    
    - Sentiment analysis, document archiving, legal discovery.
        

---

### **1.4 Spatial Data**
![[Spatial Data.png]]
- **Definition**:  
    Data that includes a geographic or spatial component.
    
- **Examples**:
    
    - GIS (Geographic Information System) data.
        
    - GPS coordinates.
        
    - Satellite images, maps.
        
- **Advantages**:
    
    - Enables location-based insights and analysis.
        
- **Limitations**:
    
    - Large storage requirements.
        
    - Needs specialized tools (ArcGIS, PostGIS).
        
- **Applications**:
    
    - Logistics optimization, city planning, environmental monitoring.
        

---

### **1.5 Time Series Data**
![[Time Series Data.png]]
- **Definition**:  
    Data collected and stored over time to capture changes and trends.
    

#### **Type 1: Regular Interval Time Series**

- **Characteristics**:
    
    - Data recorded at consistent, fixed intervals.
        
    - Easy to handle with statistical methods and time series databases.
        
- **Lecture Examples**:
    
    - Hourly weather readings.
        
    - Daily closing stock prices.
        
    - Sensor readings every second.
        
- **Advantages**:
    
    - Easy to analyze and visualize.
        
    - Supports trend detection and forecasting.
        
- **Limitations**:
    
    - Can produce redundant data if no change occurs between intervals.
        

#### **Type 2: Irregular Interval Time Series**

- **Characteristics**:
    
    - Data recorded only when an event occurs.
        
    - Time gaps between data points are inconsistent.
        
- **Lecture Examples**:
    
    - GPS tracking when position changes.
        
    - Log entries on system errors.
        
    - Sensor reports when thresholds are exceeded.
        
- **Advantages**:
    
    - Efficient storage and transmission.
        
    - Captures only relevant events.
        
- **Limitations**:
    
    - Harder to apply certain statistical models (may need resampling).
        

---

### **1.6 Graph Data**
![[Graph Data.png]]
- **Definition**:  
    Data stored as nodes (entities) and edges (relationships).
    
- **Examples**:
    
    - Social media networks (user-to-user connections).
        
    - Fraud detection (transactions and their relationships).
        
- **Advantages**:
    
    - Efficient for relationship-based queries.
        
    - Good for traversing connected data.
        
- **Limitations**:
    
    - Requires specialized databases (Neo4j, OrientDB).
        
- **Applications**:
    
    - Recommendation engines, supply chain mapping.
        

---

### **1.7 Multimedia Data**
![[Multimedia Data.png]]
- **Definition**:  
    Data in image, video, audio, or graphics format.
    
- **Examples**:
    
    - Digital photos, surveillance videos, audio recordings.
        
- **Advantages**:
    
    - Highly engaging and rich in information.
        
- **Limitations**:
    
    - Large file sizes require high storage and processing capacity.
        
- **Applications**:
    
    - Media streaming, advertising, facial recognition, voice analytics.
## 2. Data Architecture 
### 2.1 Concepts
Data Architecture is theÂ **â€œoverall blueprintâ€**Â for the entire lifecycle of data within an organization.  
It specifiesÂ **where the data comes from, where it is stored, how it is processed, and how it is ultimately used**.

If we think of data as raw materials, then Data Architecture is like a factory assembly line that ensures the raw materials go through the right paths, are processed correctly, and result in high-quality finished products.

---
### 2.2 Role
- Connects data from various sources into aÂ **unified system**.
    
- EnsuresÂ **high-quality data**Â â€“ accurate, complete, consistent, and ready for use.
    
- Supports decision-making based onÂ **processed and analyzed data**.
    
- Reduces cost and risk byÂ **optimizing data flows**Â and implementingÂ **centralized governance**
### 2.3 Main Layers in Data Architecture

#### 2.3.1 Data Sources (Nguá»“n dá»¯ liá»‡u)

This is whereÂ **all the raw data originates**.  
Think of it as the â€œinput warehouseâ€ of your factory.

- **Transactional Databases**Â â€“ Data from applications that handle day-to-day transactions (e.g., sales systems, banking systems).
    
- **Log Files**Â â€“ Records of system activity, user actions, and error tracking.
    
- **Applications**Â â€“ Business software like CRM, ERP that generate data.
    
- **IoT Sensors**Â â€“ Devices that collect environmental readings like temperature, location, machine status.
    
- **APIs**Â â€“ Data pulled from other systems or platforms via programmatic access.
    
- **Third-party Data**Â â€“ Data purchased or received from external providers, such as market research or demographic data.
    

ğŸ’¡Â _In short: This is the â€œraw ingredientâ€ stage before any cleaning or processing happens._

---

#### 2.3.2 Operational Data Layer (Lá»›p dá»¯ liá»‡u váº­n hÃ nh)

TheÂ **working area**Â where data is stored and accessed forÂ **real-time business operations**.

- Usually based onÂ **OLTP (Online Transaction Processing)**Â systems.
    
- Handles frequent updates, inserts, and queries.
    
- Focuses onÂ **speed and accuracy**Â to support day-to-day activities like order entry, banking transactions, or inventory updates.
    

ğŸ’¡Â _Analogy: This is like the â€œproduction floorâ€ where materials are actively used in daily operations._

---

#### 2.3.3 Ingestion & Staging Layer (Lá»›p tiáº¿p nháº­n vÃ  lÆ°u táº¡m)

AÂ **temporary holding area**Â for raw data before it is processed.

- **Data Ingestion**: Pulling in data from multiple sources.
    
- **Staging**: Storing that data in its raw form without altering it.
    
- Allows data engineers toÂ **validate quality**Â before it moves forward.
    
- ProtectsÂ **original, unmodified copies**Â for backup and auditing.
    

ğŸ’¡Â _Analogy: This is the â€œreceiving dockâ€ where goods arrive and are checked before being sent into production._

---

#### 2.3.4 Integration & Transformation Layer (Lá»›p tÃ­ch há»£p vÃ  chuyá»ƒn Ä‘á»•i)

This is whereÂ **data cleaning, merging, and formatting**Â happens.

- **Integration**: Combine data from multiple sources into one consistent format.
    
- **Transformation**: Change data structure, remove duplicates, standardize units, fix missing values.
    
- Ensures data isÂ **accurate, consistent, and analysis-ready**.
    

ğŸ’¡Â _Analogy: This is the â€œquality control & assembly areaâ€ where raw parts are processed into usable components._

---

#### 2.3.5 Analytics Data Layer (Lá»›p phÃ¢n tÃ­ch dá»¯ liá»‡u)

AÂ **centralized storage**Â for cleaned and structured data ready for reporting and analysis.

- Often implemented asÂ **Data Warehouse**Â orÂ **Data Mart**.
    
- StoresÂ **historical data**Â for trend analysis and strategic planning.
    
- SupportsÂ **OLAP (Online Analytical Processing)**Â for deep analysis and querying.
    

ğŸ’¡Â _Analogy: This is the â€œfinished goods warehouseâ€ where products are ready to be shipped to customers (in this case, the customers are analysts and decision-makers)._

---

#### 2.3.6 Business Intelligence (BI) Layer (Lá»›p thÃ´ng tin kinh doanh)

TheÂ **user-facing layer**Â where decision-makers interact with the data.

- ProvidesÂ **dashboards**,Â **reports**, andÂ **data visualizations**.
    
- Tools likeÂ **Power BI, Tableau, Qlik**Â help managers and staff extract insights quickly.
    
- SupportsÂ **data-driven decision making**Â at all levels of the organization.
    

ğŸ’¡Â _Analogy: This is the â€œstorefrontâ€ where the final products are displayed and delivered to customers in the most usable form._

### 2.4 Components of Data Architecture
#### 2.4.1 Data Modelling

##### 2.4.1.1 Definition
Data Modelling is the process ofÂ **identifying, organising, and representing the structure of data**Â in a system so that it is clear, consistent, and understandable for both technical teams and business stakeholders.

- It definesÂ **what data needs to be stored**,Â **how it relates to other data**, andÂ **what business rules**Â apply.
    
- Think of it as the â€œblueprintâ€ of the data system, similar to how an architectural drawing works for a building.
    

---

##### 2.4.1.2 Main Objectives

- EnsureÂ **data consistency**Â across the entire system.
    
- Provide aÂ **common language**Â for everyone involved in the project.
    
- Reduce errors during implementation by creating a complete, agreed-upon plan before coding.
    
- Create a foundation for query optimisation and data management.
    

---

##### 2.4.1.3 Three Levels of Data Modelling

###### a) Conceptual Data Model
![[Conceptual Data Model.png]]
- **Purpose:**Â Provide a big-picture view of the data in the system.
    
- **Focus:**
    
    - IdentifyÂ **entities**Â (things we store information about) andÂ **relationships**Â between them.
        
    - DoesÂ **not**Â define tables or database structures yet.
        
- **Characteristics:**
    
    - High-level, business-oriented.
        
    - Example: â€œCustomerâ€ places â€œOrdersâ€.
        
    - May show relationships such as one-to-many or many-to-many, but not the exact table design.
        
- **When used:**Â Early in the project, to align understanding with stakeholders.
    

---
###### b) Logical Data Model
![[Logical Data Model.png]]
- **Purpose:**Â Turn the conceptual model into a more technical structure, but stillÂ **not tied to any specific technology**.
    
- **Focus:**
    
    - DefineÂ **attributes**Â for each entity.
        
    - SpecifyÂ **primary keys (PK)**Â andÂ **foreign keys (FK)**Â logically.
        
    - Show relationship cardinality (1:1, 1:N, M:N).
        
- **Characteristics:**
    
    - More detail than conceptual, but independent of the database platform.
        
    - Example: â€œCustomerâ€ table hasÂ `CustomerID`,Â `Name`,Â `Email`; â€œOrderâ€ table hasÂ `OrderID`,Â `CustomerID`,Â `OrderDate`.
        
- **When used:**Â After finalising the conceptual model, to prepare for database design.
    

---

###### c) Physical Data Model
![[Physical Data Model'.png]]
- **Purpose:**Â Implement the logical model into anÂ **actual database structure**Â on a specific DBMS.
    
- **Focus:**
    
    - CreateÂ **real database tables**.
        
    - DefineÂ **exact data types**Â (`VARCHAR`,Â `INT`,Â `DATE`, etc.).
        
    - AddÂ **indexes, constraints, partitions**Â for performance optimisation.
        
    - Set storage, backup, and user permissions.
        
- **Characteristics:**
    
    - Highly technical, depends on chosen technology (e.g., MySQL, PostgreSQL, Oracle).
        
    - Example:
        
        sql
        
        CopyEdit
        
        `CREATE TABLE Customers (   CustomerID INT PRIMARY KEY,   Name VARCHAR(100),   Email VARCHAR(100) );`
        
- **When used:**Â During implementation, before starting application development.
    

---

##### 2.4.1.4 Relationship Between the Three Levels

- **Conceptual:**Â DecidesÂ **WHAT**Â to store.
    
- **Logical:**Â DecidesÂ **HOW**Â to organise data logically.
    
- **Physical:**Â DecidesÂ **HOW**Â to implement it technically in the database.
    

---

##### 2.4.1.5 Benefits of Data Modelling

- Reduces misunderstandings between business and technical teams.
    
- Makes maintenance and future expansion easier.
    
- Enables accurate reporting and analytics.
    
- Improves system performance by optimising design early.
#### 2.4.2 Data Ingestion 
##### 2.4.2.1 Definition 
Data Ingestion isÂ **the process of collecting and bringing data from multiple sources**Â into a centralized storage location (Data Destination) such as a Data Lake, Data Warehouse, or analytics platform for storage, processing, and analysis.

- It is theÂ **first step**Â in a data pipeline.
    
- Can happen inÂ **batch mode**Â orÂ **real-time/streaming**Â mode.
##### 2.4.2.2 Purpose

- BringÂ **data from various sources**Â (databases, IoT devices, APIs, applications, external providers) into a centralised system.
    
- Ensure data isÂ **captured in a consistent, accurate, and timely manner**.
    
- SupportÂ **real-time decision-making**Â orÂ **batch processing**Â for reports and analytics.
    
---

##### 2.4.2.3 Types of Data Ingestion

1. **Batch Ingestion**
    
    - Data is collected and loaded atÂ **scheduled intervals**Â (e.g., hourly, daily).
        
    - Best for large volumes of data that donâ€™t require instant updates.
        
    - Example: Loading daily sales transactions from a storeâ€™s POS system to a central data warehouse at midnight.
        
2. **Real-Time (Streaming) Ingestion**
    
    - Data is ingestedÂ **continuously**Â as it is created or updated.
        
    - Useful for applications that requireÂ **instant insights**.
        
    - Example: Live stock price updates, IoT sensor readings, or social media feeds.
        
3. **Micro-Batch Ingestion**
    
    - Hybrid approach: very small batches processed frequently (e.g., every few minutes).
        
    - Balances speed with processing efficiency.
#####  2.4.2.4 General Workflow
Data ingestion typically follows one of two main patterns:Â **ETL**Â orÂ **ELT**
###### a)Â ETL (Extract â€“ Transform â€“ Load)
**Definition of ETL (Extract â€“ Transform â€“ Load)**  
	ETL is a traditional data processing model, especially common for loading data into anÂ **Enterprise Data Warehouse (EDW)**.  
	The process consists of three main steps:
	- **Extract**
		- Copy data from various source systems into aÂ **staging area**.
		- Usually performed inÂ **batch mode**Â (e.g., every hour, every day).
	- **Transform**
		- **Data cleansing**Â â†’ remove errors, duplicates, and missing values.		
		- **Data enrichment**Â â†’ add necessary supplementary information.
		- Convert structure and format so the data fits theÂ **data warehouse schema**.
	- **Load**
		- Load the processed data into the tables of theÂ **data warehouse**.
		
---

**Limitations of the ETL Model**
- Suitable only forÂ **batch processing**Â â†’ not optimized forÂ **real-time data processing**.

- Rigid schema in EDW â†’ difficult to change or adapt when the source data structure changes.
    
- Does not supportÂ **unstructured data**Â â†’ data without a schema (unstructured) or binary data is difficult to load into EDW.
    
- Loss of data that cannot be loaded â†’ if data canâ€™t be loaded into EDW, it becomes unavailable for analysis.
    
- Loss ofÂ **lineage information**Â (data origin) â†’ once loaded into EDW, the source history is not retained, making backtracking difficult.
    
---
**When to Use ETL?**

- When data isÂ **stable**Â and schema changes are rare.
    
- WhenÂ **real-time analytics**Â is not required.
    
- When strictÂ **data control and standardization**Â is needed before storage.
######  b)Â ELT (Extract â€“ Load â€“ Transform)
**Definition of ELT**  
ELT is a variation of ETL, but the processing order changes: instead ofÂ **Extract â†’ Transform â†’ Load**, ELT performsÂ **Extract â†’ Load â†’ Transform**.

**Steps in ELT:**

- **Extract and Load**
    
    - Data from multiple source systems is extracted and directly loaded into aÂ **centralized repository**, often in the form of aÂ **data lake**.
        
    - The data is stored in itsÂ **raw form**Â immediately upon loading, without any processing.
        
- **Transform**
    
    - When analysis is needed, the data in the data lake isÂ **cleansed**Â (error removal, handling missing values),Â **enriched**Â (adding supplemental data), andÂ **transformed**Â (structural and format changes).
        
    - The processed data is then sent to the most appropriate location for analysis, for example:
        
        - **Data Warehouse**Â (with clear tabular structure)
            
        - **NoSQL Database**Â (for unstructured data)
            

---

 **Advantages of ELT**

- **Preserves Raw Data**Â â€“ Keeps all original data in the data lake, protecting historical records andÂ **data lineage**(origin tracking).
    
- **Supports All Data Types**Â â€“ Works with structured, semi-structured (JSON, XML), unstructured (text, images, videos), and binary data.
    
- **Flexible**Â â€“ You can transform data only when needed, rather than before loading.
    
- **Faster Loading**Â â€“ Since transformation is skipped during ingestion, large datasets can be loaded quickly.
    
- **Real-time Friendly**Â â€“ Works well with streaming and real-time ingestion.
    

---

 **When to Use ELT**

- When you haveÂ **large volumes**Â of data from many sources.
    
- When you need toÂ **retain raw historical data**Â for auditing, compliance, or future analysis.
    
- When your analytics environment needs toÂ **handle diverse data types**Â (structured + unstructured).
    
- When you useÂ **modern cloud-based platforms**Â that can store and process raw data at scale (e.g., Snowflake, Databricks, BigQuery).
###### c) ETL vs ELT 
 **1. Processing Order**
- **ETL**: Extract â†’Â **Transform**Â â†’ Load
    
    - Data is cleaned and transformedÂ **before**Â loading into the target system.
        
- **ELT**: Extract â†’Â **Load**Â â†’ Transform
    
    - Data is loadedÂ **first**Â in its raw form, then transformedÂ **later**Â when needed.
        

---
 **2. Where the Transformation Happens**

- **ETL**: In aÂ **separate ETL processing engine**Â before the data reaches the data warehouse.
    
- **ELT**: Inside theÂ **target system**Â (data lake, cloud warehouse) after data is already stored.
    

---
 **3. Storage of Raw Data**

- **ETL**: Raw data isÂ **not stored**Â â€” only processed data is kept in the data warehouse.
    
- **ELT**: Raw data isÂ **stored and preserved**, which means you keep full historical records and lineage.
    

---
 **4. Data Types Supported**

- **ETL**: MostlyÂ **structured data**Â (tabular, fixed schema).
    
- **ELT**: Can handleÂ **structured, semi-structured, unstructured, and binary**Â data.
    

---
 **5. Speed and Flexibility**

- **ETL**: Slower to load large datasets because transformation happens first.
    
- **ELT**: Faster to load because transformation is skipped during ingestion, and more flexible since transformation can be done anytime.
    

---
 **6. Real-Time Capability**

- **ETL**: Mainly batch processing, not optimized for real-time.
    
- **ELT**: Works well withÂ **real-time and streaming**Â data ingestion.
##### 2.4.2.5 Data Sources
Common types include:

- **Apache Kafka**Â (streaming platform)
    
- **JDBC**Â (database connectors)
    
- **Oracle CDC**Â (Change Data Capture)
    
- **HTTP Clients**Â (API data ingestion)
    
- **HDFS**Â (Hadoop Distributed File System)
    

---

##### 2.4.2.5 Data Destinations  
Common destinations include:

- **Apache Kafka**Â (for streaming pipelines)
    
- **JDBC**Â (to write into databases)
    
- **Snowflake**Â (cloud data warehouse)
    
- **Amazon S3**Â (cloud object storage)
    
- **Databricks**Â (big data analytics platform)

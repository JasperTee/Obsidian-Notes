## 1. Conceptualising interaction
### 1. Mental Models

#### 1. Definition

- A **mental model** is the way a user **thinks in their mind** about how a system, product, or device works.
    
- It’s like a **mental map** that people build based on their **experiences, observations, and assumptions**.
    

---

#### 2. Characteristics of Mental Models

- **Personal** – each person develops their own mental model of the same system.
    
- **Incomplete** – often built from partial experience and not fully accurate.
    
- **Flawed** – can be based on misunderstandings or false assumptions.
    
- **Dynamic** – changes over time as users learn or gain new experiences.
    

---

#### 3. Why are Mental Models important in design?

- Users act according to their **mental models**, not according to the designer’s system model.
    
- If their mental model doesn’t match how the system really works → confusion, mistakes, frustration.
    
- Good design should:
    
    - **Understand** users’ existing mental models.
        
    - **Align** the conceptual model (designer’s intended model) with those mental models.
        
    - Use **familiar metaphors, clear feedback, and intuitive language** to help users form accurate understandings.
        

---

#### 4. Real-World Examples

1. **Traffic light button**:
    
    - Many people believe pressing the button multiple times makes the light change faster.
        
    - This is a **wrong mental model**, but very common.
        
2. **Recycle Bin on computers**:
    
    - Users believe deleting a file moves it into the “trash,” where it can be restored.
        
    - This is a **correct mental model**, reflecting real-life trash bins.
        
3. **Save icon (floppy disk)**:
    
    - Many younger users have never seen a floppy disk, but still recognize it as “save.”
        
    - This shows how **mental models persist through familiar symbols**.
        

---

#### 5. Connection to Design

- Designers should **leverage users’ existing mental models** when possible, to make systems intuitive.
    
- If users’ mental models are **wrong**, designers must use **conceptual models** (through interface design, feedback, instructions) to reshape them correctly.

---

### 2. Conceptual Models

- #### 1. Definition

- A **conceptual model** is the model that **designers create** to explain how a system works and how users should interact with it.
    
- It is not the mental model in the user’s head; rather, it is the **“intended design idea”** that designers communicate through the interface, features, and feedback.
    

---

#### 2. Role of Conceptual Models

- **Guide users** – help them understand how the system works.
    
- **Shape mental models** – users form their mental models based on the conceptual model presented.
    
- **Increase intuitiveness and learnability** – a clear conceptual model makes products easier to use, even for beginners.
    
- **Connect to real life** – uses metaphors, familiar language, and symbols to reduce the learning curve.
    

---

#### 3. Characteristics

- **Abstract representation** – not about technical details, but about how the user perceives and uses the system.
    
- **Represents designer’s intent** – shows how designers want users to think about and interact with the system.
    
- **Grounded in design principles** – e.g., affordances, mapping, feedback, visibility, and constraints.
    

---

#### 4. Real-World Examples

1. **Shopping cart in e-commerce**
    
    - Adding products into a “cart” mirrors real-life shopping.
        
    - Helps align user mental models with the digital system.
        
2. **Calendar apps**
    
    - Saving an event in an online calendar is like writing in a diary.
        
    - The conceptual model makes the system intuitive.
        
3. **Drag-and-drop**
    
    - Dragging a file into the “Recycle Bin” to delete it.
        
    - Reflects real-world experiences → intuitive and easy.
        

---

#### 5. Relationship with Mental Models

- **Mental model** = what users believe about how the system works.
    
- **Conceptual model** = what designers intend to communicate.
    
- A good conceptual model ensures that **users build accurate mental models** of the system.
    

---

#### 6. Principles for Creating Conceptual Models

1. **Match familiar mental models** – leverage what users already know.
    
2. **Reduce cognitive load** – use metaphors and plain language.
    
3. **Provide clear feedback** – so users know the system is responding.
    
4. **Maintain consistency** – avoid confusion by keeping patterns predictable.
---

### 3. Interaction Types (Preece et al., 2015)

#### 1. Concept

- Interaction types describe the **ways users interact with a system** – the methods they use to communicate with and control a product or application.
    
- Preece et al. (2015) identify **four main types**: **Instructing, Conversing, Manipulating, and Exploring**.
    

---

#### 2. The Four Types

##### a. Instructing

- **Definition**: Users give **direct commands** to the system via menus, keyboard, buttons, or voice.
    
- **Characteristics**:
    
    - Fast and efficient.
        
    - Suitable for **repetitive** or **precise** tasks.
        
- **Examples**:
    
    - Clicking “Print” to print a file.
        
    - Typing commands in a terminal.
        
    - Saying “Turn off the lights” to a voice assistant.
        

---

##### b. Conversing

- **Definition**: Users interact with the system as if having a **conversation with another person**.
    
- **Characteristics**:
    
    - Two-way communication.
        
    - Natural, approachable; useful for people with little technical experience.
        
- **Examples**:
    
    - Customer service chatbots.
        
    - Siri, Alexa, Google Assistant.
        
    - Voice user interfaces (VUIs).
        

---

##### c. Manipulating

- **Definition**: Users **directly act** on objects (virtual or physical) through dragging, dropping, rotating, zooming, resizing.
    
- **Characteristics**:
    
    - Intuitive, easy to learn because it mimics real-world actions.
        
    - Provides a sense of “grabbing and controlling” objects.
        
- **Examples**:
    
    - Dragging a file into the Recycle Bin.
        
    - Pinch-to-zoom on a smartphone.
        
    - Using VR gloves or gesture-based controllers.
        

---

##### d. Exploring

- **Definition**: Users **navigate and move around** a physical or virtual environment to discover and learn.
    
- **Characteristics**:
    
    - Immersive, engaging.
        
    - Useful for learning, entertainment, and navigating complex environments.
        
- **Examples**:
    
    - Walking through a city in Google Maps Street View.
        
    - Playing open-world games like Assassin’s Creed or Minecraft.
        
    - Exploring 3D worlds in VR with Oculus Rift or HoloLens.
        

---

#### 3. Importance in Design

- Each interaction type has strengths and weaknesses, making it suitable for different contexts:
    
    - **Instructing** → best for quick, repetitive actions (e.g., adjusting volume, turning devices on/off).
        
    - **Conversing** → helpful for natural interactions, accessibility, and beginners.
        
    - **Manipulating** → powerful for design, drawing, or simulation tasks.
        
    - **Exploring** → ideal for discovery, learning, or immersive experiences.
        
- In practice, **systems often combine multiple interaction types** to provide a rich, flexible user experience.
        

---
## 2. Input and Output Technologies

### 1. Why are Input/Output important?

- Interaction with computers is a **feedback loop**:
    
    - Human actions (output) → System input.
        
    - System processing → System output → Human input.
        
- Choosing the right input and output devices affects:
    
    - **Efficiency** – can users complete their tasks effectively?
        
    - **Experience** – is the interaction smooth, enjoyable, meaningful?
        
    - **Accessibility** – does it suit people with different physical or cognitive abilities?
        

---

### 2. Input Devices

- **Definition**: A device (with software) that transforms user actions into data the system can process.
    
- **Functions**:
    
    - Issue instructions (commands).
        
    - Enter and register data.
        
    - Communicate user intentions in a way computers can interpret.
        

#### Examples

1. **Traditional input**:
    
    - Keyboards (physical, on-screen).
        
    - Pointing devices: mouse, joystick, trackpad, game controllers.
        
2. **Composite devices**:
    
    - Touchscreens, smartphones (combine input + output).
        
3. **Audio/voice input**:
    
    - Microphones, speech recognition, biometric voice verification.
        
4. **Imaging and sensors**:
    
    - Cameras, scanners, Kinect, motion sensors, EEG, eye-tracking, heat sensors.
        
5. **Specialized/assistive input**:
    
    - Foot mouse (for people with mobility limitations).
        
    - Eye-movement tracking (for people with disabilities).
        
    - Stephen Hawking’s infrared cheek switch.
        

#### Design Considerations

- Match **physiology** (hand size, motor skills, reach, strength).
    
- Match **cognitive abilities** (memory, attention).
    
- Fit **tasks and environment**:
    
    - Noisy environment → touch better than speech.
        
    - Sun glare → physical buttons better than screens.
        
- Support **safe, efficient, enjoyable** interactions.
    

---

### 3. Output Devices

- **Definition**: Convert computer data into forms perceivable by humans.
    
- **Traditional output**: was only visual (on screen/paper), now includes many modalities.
    

#### Examples

1. **Visual output**:
    
    - Screens, projectors, LED panels, AR/VR headsets.
        
2. **Auditory output**:
    
    - Speakers, headphones, voice feedback, chatbots.
        
3. **Haptic output**:
    
    - Vibrations, force feedback, robotic actuators, VR gloves.
        
4. **Tangible output**:
    
    - 3D printing, physical models (objects, organs, houses).
        
5. **Motion-based output**:
    
    - Robots, moving devices, mechanical systems.
        

#### Design Considerations

- Does it support the user’s **abilities** (e.g., audio output for visually impaired)?
    
- Does it match the **task/context** (e.g., auditory output while driving)?
    
- Does it improve **usability** by providing clear feedback?
    

---

### 4. Emerging Input/Output Technologies

#### 1. Brain-Computer Interfaces (BCI)

- **Definition**: Technology that allows the **brain to communicate directly with computers** using neural signals.
    
- **Examples**:
    
    - **Neuralink** (Elon Musk): implanted chip enabling people with paralysis to move a cursor with their thoughts.
        
    - **Brain-controlled robots**: controlling devices or prosthetics through brain activity.
        
- **Impact**:
    
    - Assists people with severe disabilities.
        
    - Opens possibilities for human enhancement.
        

---

#### 2. Wearable Sensors for Healthcare

- **Definition**: Body-worn devices that **monitor health in real time**.
    
- **Examples**:
    
    - Smartwatches measuring heart rate, sleep, blood oxygen.
        
    - IoT healthcare systems: sensors tracking heart rate + room conditions, sending data to doctors remotely.
        
- **Impact**:
    
    - Enables telemedicine (remote healthcare).
        
    - Supports proactive healthcare and early diagnosis.
        

---

#### 3. Bio-Digital Systems

- **Definition**: Integration of **biotechnology and digital technology** in wearable devices.
    
- **Examples**:
    
    - Wearable labs analyzing hormones, microbiomes, or genes directly on the body.
        
- **Impact**:
    
    - Moves from simply **measuring** health to **optimizing** it.
        
    - Supports personalized medicine.
        

---

#### 4. Augmented Reality (AR) & Mixed Reality (MR) Devices

- **Definition**: Devices that **merge the real and virtual worlds**, letting users see reality while interacting with digital content.
    
- **Examples**:
    
    - **Google Glass, Ray-Ban Meta smart glasses**: display information in the user’s view.
        
    - **Microsoft HoloLens, Vive XR Elite**: immersive MR with 3D objects in real spaces.
        
- **Impact**:
    
    - Useful in education, training, design, and healthcare.
        
    - Provides immersive, intuitive interaction.
        

---

#### 5. Speech and Voice Interfaces (with AI integration)

- **Definition**: Communicating with systems through speech, enhanced by AI for recognition and response.
    
- **Examples**:
    
    - Siri, Alexa, Google Assistant (improved accuracy with machine learning).
        
    - **WeChat “Voice Donor”** project: donors record voices to create synthetic speech for people who lost theirs.
        
- **Impact**:
    
    - Hands-free, eyes-free operation (e.g., while driving).
        
    - Improves accessibility for visually impaired users.
        

---

#### 6. Tangible and Motion-Based Output

- **Definition**: Outputs that users can **feel or physically interact with**, beyond visuals or audio.
    
- **Examples**:
    
    - 3D printing → turning digital data into real objects.
        
    - Haptic gloves, VR controllers → simulate touch and force.
        
    - Motion-based robots → physical responses to digital input.
        
- **Impact**:
    
    - Expands experience from “seeing and hearing” to “touching and feeling.”
        
    - Useful in remote surgery, vocational training, VR gaming.
        

---